{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets --quiet\n"
      ],
      "metadata": {
        "id": "d8ATIoijLFBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We install the transformers library for using GPT-2 and datasets for loading and managing datasets."
      ],
      "metadata": {
        "id": "HmJna0ksLH01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    GPT2Tokenizer, GPT2LMHeadModel,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    Trainer, TrainingArguments)\n"
      ],
      "metadata": {
        "id": "1PuH_baRLRW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We import essential tools:\n",
        "GPT2Tokenizer & GPT2LMHeadModel for tokenization and model usage\n",
        "Trainer and TrainingArguments for model training (even though training isn't fully shown)\n",
        "load_dataset to read our custom text data"
      ],
      "metadata": {
        "id": "_3sJh-QWLXej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
      ],
      "metadata": {
        "id": "8UYI09R9LaVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Disabling  Weights & Biases logging"
      ],
      "metadata": {
        "id": "piGEUbB4LhWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n"
      ],
      "metadata": {
        "id": "0UyvtMkeLlJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load the pre-trained GPT-2 model and tokenizer. Since GPT-2 has no padding token by default, we set the padding token to the end-of-sequence token."
      ],
      "metadata": {
        "id": "ShSC0bTkLqmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lyrics_file = \"lyrics.txt\"\n",
        "if not os.path.exists(lyrics_file):\n",
        "    sample_lyrics = [\n",
        "        \"You're the one that I want\\n\",\n",
        "        \"Hello from the other side\\n\",\n",
        "        \"Cause baby you're a firework\\n\",\n",
        "        \"Let it go, let it go\\n\",\n",
        "        \"We will, we will rock you\\n\"\n",
        "    ]\n",
        "    with open(lyrics_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.writelines(sample_lyrics)\n"
      ],
      "metadata": {
        "id": "J_LR7bK2Lr4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a simple lyrics file with 5 famous English song lines. This serves as a small sample dataset."
      ],
      "metadata": {
        "id": "oJDBdFGALvcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"text\", data_files={\"train\": lyrics_file})\n"
      ],
      "metadata": {
        "id": "BguhSkDZLyuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load the lyrics text file into Hugging Face’s datasets format so we can tokenize and preprocess it"
      ],
      "metadata": {
        "id": "eQDCcqgWL4IM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_text(example):\n",
        "    return tokenizer(example[\"text\"], truncation=True)\n",
        "\n",
        "tokenized_data = dataset.map(tokenize_text, batched=True)\n"
      ],
      "metadata": {
        "id": "V9w0ZqP-L5Py"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We tokenize the text into numerical input IDs using GPT-2’s tokenizer. Tokenization is required to convert human-readable text into a format the model can understand."
      ],
      "metadata": {
        "id": "NkK87sPYL8cd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-lyrics-output\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=2,\n",
        "    save_steps=100,\n",
        "    logging_steps=10,\n",
        "    save_total_limit=1,\n",
        "    prediction_loss_only=True\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZqjk06vL_ZO",
        "outputId": "08cc1e0a-3c71-4b5f-dab9-1f0d97baa344"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This sets up training configurations. Even though actual training isn’t performed in this version, this step is included for completeness in case training is added later."
      ],
      "metadata": {
        "id": "d0DFZ__5MHJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_lyrics(prompt, max_new_tokens=100):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "    output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=1.0,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        repetition_penalty=1.2,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return generated_text\n"
      ],
      "metadata": {
        "id": "6Ge9luTFMILv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function takes a song prompt and generates song lyrics using the GPT-2 model.\n",
        "\n",
        "temperature, top_k, top_p control randomness and diversity\n",
        "\n",
        "repetition_penalty reduces repetitive words\n",
        "\n",
        "max_new_tokens is the number of new tokens to generate"
      ],
      "metadata": {
        "id": "RyFaPZTxMNPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"gpt2-lyrics-model\")\n",
        "tokenizer.save_pretrained(\"gpt2-lyrics-model\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLnxyGzzMOGR",
        "outputId": "9a273981-1e05-495b-b861-ddad155c9069"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('gpt2-lyrics-model/tokenizer_config.json',\n",
              " 'gpt2-lyrics-model/special_tokens_map.json',\n",
              " 'gpt2-lyrics-model/vocab.json',\n",
              " 'gpt2-lyrics-model/merges.txt',\n",
              " 'gpt2-lyrics-model/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saves the current model and tokenizer locally so they can be reused without re-downloading or re-training."
      ],
      "metadata": {
        "id": "eN4XiwBXMUWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_prompt = input(\" Enter your song prompt: \")\n",
        "lyrics = generate_lyrics(user_prompt)\n",
        "print(\"\\n Generated Lyrics:\")\n",
        "print(lyrics)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUOqa9NxMVHV",
        "outputId": "e4146526-47e2-41df-9760-011e3d0a09aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Enter your song prompt: Under the moonlight, we dance alone\n",
            "\n",
            " Generated Lyrics:\n",
            "Under the moonlight, we dance alone. We feel that in our hearts it is necessary for us to live on Earth and have an emotional life with a loving partner; after all this time has passed I am happy because God gave me there-for free from hunger but now he keeps feeding every drop of food off my plate.\" In other words:\n",
            "It's been 3 weeks since her husband left Hawaii — she can't walk (even though they are pretty far along), so when does another illness catch up? She still knows what will\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Takes input from the user (e.g., a line like “Under the moonlight, we dance alone  \n",
        "The stars above are shining bright  \n",
        "I feel your hand, I hear your tone  \n",
        "Together we escape the night...\n",
        "”) and generates a continuation using the GPT-2 model."
      ],
      "metadata": {
        "id": "Q_frFTUfMeeG"
      }
    }
  ]
}